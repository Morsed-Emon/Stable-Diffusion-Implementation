{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10052960,"sourceType":"datasetVersion","datasetId":6194301}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q datasets diffusers transformers accelerate torchmetrics[image]\n#@title Install the required libs\n!pip install -U -qq git+https://github.com/huggingface/diffusers.git\n!pip install -qq accelerate tensorboard transformers ftfy gradio\n!pip install -qq \"ipywidgets>=7,<8\"\n!pip install -qq bitsandbytes\n!pip install huggingface_hub\n!huggingface-cli login --token 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\n\nimport os\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torchmetrics.image.fid import FrechetInceptionDistance\n#from torchmetrics.image.kid import KernelInceptionDistance\nfrom torchmetrics.image.kid import KernelInceptionDistance as KID\nfrom torchmetrics.multimodal import CLIPScore\nfrom diffusers import StableDiffusionPipeline\nimport pandas as pd  # Ensure you have imported pandas\nfrom transformers import CLIPProcessor, CLIPModel\nfrom torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\nimport torchvision.transforms.functional as TF","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T18:43:56.266512Z","iopub.execute_input":"2025-03-27T18:43:56.266831Z","iopub.status.idle":"2025-03-27T18:45:05.121578Z","shell.execute_reply.started":"2025-03-27T18:43:56.266796Z","shell.execute_reply":"2025-03-27T18:45:05.120553Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.1/124.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.9/246.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2025.1.31)\nThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nThe token `MorsedEmon123` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `MorsedEmon123`\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom torchmetrics.image.fid import FrechetInceptionDistance\nfrom torchmetrics.image.kid import KernelInceptionDistance\nfrom torchmetrics.image.psnr import PeakSignalNoiseRatio\nfrom torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\nfrom diffusers import StableDiffusionPipeline\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\n\n# Transformation for real images (optimized for FID, KID)\ntransform = transforms.Compose([\n    transforms.Resize((299, 299), interpolation=Image.BICUBIC),  # Resize before Tensor conversion\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n\n# Function to correct dataset paths (Kaggle-specific)\ndef correct_real_image_paths(real_image_paths):\n    corrected_paths = [\n        path.replace(\n            '/kaggle/input/casia-iris-thousand',\n            '/kaggle/input/iris-thousand-dataset/CASIA-Iris-Thousand'\n        )\n        for path in real_image_paths\n    ]\n    return corrected_paths\n\n# Function to generate images in batches\ndef generate_images_batch(prompts, pipeline, batch_size=4, device=\"cuda\"):\n    all_generated_images = []\n    num_batches = len(prompts) // batch_size + int(len(prompts) % batch_size > 0)\n    \n    for batch_idx in range(num_batches):\n        batch_prompts = prompts[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n        \n        with torch.no_grad():\n            batch_images = pipeline(batch_prompts).images\n        \n        all_generated_images.extend(batch_images)\n        \n        # Free memory\n        del batch_images\n        torch.cuda.empty_cache()\n    \n    return all_generated_images\n\n# Function to calculate FID score with batch processing\ndef calculate_fid_score(real_image_paths, generated_images, device='cuda', batch_size=4):\n    real_image_paths = correct_real_image_paths(real_image_paths)\n    fid = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n\n    for i in range(0, len(real_image_paths), batch_size):\n        real_batch, gen_batch = [], []\n        \n        for path in real_image_paths[i:i+batch_size]:\n            img = Image.open(path).convert('RGB')\n            real_batch.append(transform(img).unsqueeze(0).to(device).half())\n\n        for img in generated_images[i:i+batch_size]:\n            gen_batch.append(transform(img).unsqueeze(0).to(device).half())\n\n        real_batch = torch.cat(real_batch, dim=0)\n        gen_batch = torch.cat(gen_batch, dim=0)\n\n        fid.update(real_batch, real=True)\n        fid.update(gen_batch, real=False)\n\n        del real_batch, gen_batch\n        torch.cuda.empty_cache()\n\n    fid_score = fid.compute().item()\n    del fid\n    torch.cuda.empty_cache()\n    \n    return fid_score\n\n# Function to calculate CLIP Score\ndef calculate_clip_score(prompts, generated_images, device=\"cuda\"):\n    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)\n\n    total_score = 0.0\n    for prompt, img in zip(prompts, generated_images):\n        if isinstance(img, torch.Tensor):\n            img = transforms.ToPILImage()(img)\n\n        inputs = clip_processor(text=prompt, images=img, return_tensors=\"pt\", padding=True).to(device)\n        outputs = clip_model(**inputs)\n\n        similarity = (outputs.image_embeds @ outputs.text_embeds.T) / (\n            outputs.image_embeds.norm(dim=-1, keepdim=True) * outputs.text_embeds.norm(dim=-1, keepdim=True)\n        )\n        \n        total_score += similarity.item()\n\n    avg_clip_score = total_score / len(generated_images)\n    del clip_model, clip_processor\n    torch.cuda.empty_cache()\n    \n    return avg_clip_score\n\n# Function to calculate KID score with memory optimizations\ndef calculate_kid_score(real_image_paths, generated_images, device=\"cuda\", batch_size=4):\n    real_image_paths = correct_real_image_paths(real_image_paths)\n    kid = KernelInceptionDistance(subset_size=len(real_image_paths)).to(device)\n\n    for i in range(0, len(real_image_paths), batch_size):\n        real_batch, gen_batch = [], []\n\n        for path in real_image_paths[i:i+batch_size]:\n            img = Image.open(path).convert(\"RGB\").resize((512, 512))\n            real_batch.append(transforms.ToTensor()(img).unsqueeze(0).to(device).half())\n\n        for img in generated_images[i:i+batch_size]:\n            gen_batch.append(transforms.ToTensor()(img).unsqueeze(0).to(device).half())\n\n        real_batch = torch.cat(real_batch, dim=0)\n        gen_batch = torch.cat(gen_batch, dim=0)\n\n        # Keep images in half-precision and only convert to uint8 during the update\n        kid.update((real_batch * 255).to(torch.uint8), real=True)\n        kid.update((gen_batch * 255).to(torch.uint8), real=False)\n\n        del real_batch, gen_batch\n        torch.cuda.empty_cache()\n\n    kid_mean, kid_std = kid.compute()\n    del kid\n    torch.cuda.empty_cache()\n\n    return kid_mean.item(), kid_std.item()\n\n\n# Function to compute PSNR and SSIM\ndef calculate_psnr_ssim(real_image_paths, generated_images, device=\"cuda\"):\n    real_image_paths = correct_real_image_paths(real_image_paths)\n\n    psnr = PeakSignalNoiseRatio().to(device)\n    ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n\n    total_psnr, total_ssim = 0.0, 0.0\n    num_images = len(real_image_paths)\n\n    for real_path, gen_img in zip(real_image_paths, generated_images):\n        real_img = Image.open(real_path).convert(\"RGB\").resize((512, 512))\n        real_tensor = TF.to_tensor(real_img).unsqueeze(0).to(device)\n        gen_tensor = TF.to_tensor(gen_img).unsqueeze(0).to(device)\n\n        total_psnr += psnr(gen_tensor, real_tensor).item()\n        total_ssim += ssim(gen_tensor, real_tensor).item()\n\n    del psnr, ssim\n    torch.cuda.empty_cache()\n\n    return total_psnr / num_images, total_ssim / num_images\n\n# Dataset-specific function to load real image paths and prompts\ndef load_data_for_fid():\n    data = pd.read_csv('/kaggle/input/iris-thousand-dataset/iris_thousands.csv')\n    selected_data = data.sample(2000)  # Select 1000 images\n\n    real_image_paths = selected_data['ImagePath'].tolist()\n    labels = selected_data['Label'].tolist()\n\n    prompts = [f\"Conditioned on label: {label}\" for label in labels]\n\n    return real_image_paths, prompts\n\n# Main function to compute all scores\ndef main():\n    device = \"cuda\"\n    weight_dtype = torch.float16\n\n    # Load Stable Diffusion model\n    pipeline = StableDiffusionPipeline.from_pretrained(\n        \"Emon59/Iris_image_stable_diffusion1\", torch_dtype=weight_dtype\n    ).to(device)\n\n    real_image_paths, prompts = load_data_for_fid()\n    generated_images = generate_images_batch(prompts, pipeline)\n\n    print(f\"FID Score: {calculate_fid_score(real_image_paths, generated_images, device)}\")\n    print(f\"CLIP Score: {calculate_clip_score(prompts, generated_images, device)}\")\n    kid_mean, kid_std = calculate_kid_score(real_image_paths, generated_images, device)\n    print(f\"KID Score: {kid_mean} ± {kid_std}\")\n    psnr_score, ssim_score = calculate_psnr_ssim(real_image_paths, generated_images, device)\n    print(f\"PSNR Score: {psnr_score}\")\n    print(f\"SSIM Score: {ssim_score}\")\n\n    del pipeline\n    torch.cuda.empty_cache()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T18:45:05.123556Z","iopub.execute_input":"2025-03-27T18:45:05.124377Z","execution_failed":"2025-03-27T19:28:08.556Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3262689fdb0c47e3a5522bd953b5d6a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35764bc025614214acf501a37ca4db6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a4681d08751491a9435498166a0d1be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler_config.json:   0%|          | 0.00/379 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22cb4cd5a4f14bc2b348057aef4aac06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"324f08761ab04f27bcb730561ec3813c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/585 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4f486c1f13945ae91bb96b89aa0ac1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/630 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dc051195ef042a6b7ae331623650f1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/744 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56ca701bda344596b3a05b0965fb824e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcf334769fa043a489d5f0186522e6e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75c74e3aff0e45769fa7659ca0978afb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6857494d181648d7b17a655905abdeb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/830 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74da31fa1c634b8cac556a0be3d3d4e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"467e062145e54b7ba99d4cfdc65bb75f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18b3625d1cb141b392a737c75907d833"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7edc55dea21e4e7bbf0158b2c3a7decb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c991e37036ce4e8487265bc7bfa4fc7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb1de5dd29f54e988405e7e3675c70a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"047f7921a7dc4165b4a6018339dced48"}},"metadata":{}}],"execution_count":null}]}